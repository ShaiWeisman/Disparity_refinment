{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DenseMapNet_JFCode.ipynb","version":"0.3.2","provenance":[{"file_id":"11nhFMRWV-Q8R4NhgiFgaJWw5D90o9fdh","timestamp":1550765860060}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"-40aJp21vv-a","colab_type":"text"},"cell_type":"markdown","source":["#1.Mount Drive"]},{"metadata":{"id":"QnTZBp9u-sfl","colab_type":"code","outputId":"f49eb2fc-8cfa-47fe-a879-9005358cb021","executionInfo":{"status":"ok","timestamp":1550836724474,"user_tz":-120,"elapsed":36844,"user":{"displayName":"Shai Weisman","photoUrl":"","userId":"16331474504044317514"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"cell_type":"code","source":["from google.colab import drive # access data in Google Drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"metadata":{"id":"NkL2CfjQv2M2","colab_type":"text"},"cell_type":"markdown","source":["#2. Densemap Net\n","\n","2.1 Import libaries"]},{"metadata":{"id":"DP_W7YVmhuvn","colab_type":"code","colab":{}},"cell_type":"code","source":["import time\n","\n","class ElapsedTimer(object):\n","    def __init__(self):\n","        self.start_time = time.time()\n","    def elapsed(self, sec):\n","        if sec < 0:\n","            sec = \"%0.2f\" % (sec * 100)\n","            return sec + \" msec\"\n","        elif sec < 60:\n","            sec = \"%0.4f\" % sec\n","            return sec + \" sec\"\n","        elif sec < (60 * 60):\n","            sec = \"%0.4f\" % (sec / 60)\n","            return sec + \" min\"\n","        else:\n","            sec = \"%0.4f\" % (sec / (60 * 60))\n","            return sec + \" hr\"\n","    def elapsed_time(self):\n","        delta = time.time() - self.start_time\n","        return delta\n","\n","    def print_elapsed_time(self):\n","        print(\"Speed: %s \" % self.elapsed(self.elapsed_time()))\n","\n","class Settings(object):\n","    def __init__(self):\n","        self.ydim = 540\n","        self.xdim = 960\n","        self.channels = 3        \n","        self.model_weights = None\n","\n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1zS-tT3gEjro","colab_type":"text"},"cell_type":"markdown","source":["#2.2 model"]},{"metadata":{"id":"vqkXIiOzEaey","colab_type":"code","outputId":"49472627-595d-4613-853d-42253ac3dfd3","executionInfo":{"status":"ok","timestamp":1550840378750,"user_tz":-120,"elapsed":918,"user":{"displayName":"Shai Weisman","photoUrl":"","userId":"16331474504044317514"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["\n","import matplotlib.pyplot as plt\n","#from keras.callbacks import *\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","import imageio\n","import keras\n","from keras.layers import Dense, Dropout\n","from keras.layers import Input, Conv2D, Conv2DTranspose\n","from keras.layers import ZeroPadding2D, BatchNormalization, Activation\n","from keras.layers import UpSampling2D \n","from keras.optimizers import RMSprop\n","from keras.callbacks import ModelCheckpoint, LambdaCallback\n","from keras.models import load_model, Model\n","from keras.layers.pooling import MaxPooling2D\n","from keras.utils import plot_model\n","\n","import numpy as np\n","#from utils import Settings\n","\n","\n","class DenseMapNet(object):\n","    def __init__(self, settings):\n","        self.settings =settings\n","        self.xdim = self.settings.xdim \n","        self.ydim = self.settings.ydim\n","        self.channels = self.settings.channels\n","        self.model = None\n","\n","    def build_model(self, lr=1e-3):\n","        dropout = 0.2\n","      \n","        shape=(None,540, 960,3)#self.ydim, self.xdim, self.channels)\n","        left = Input(batch_shape=shape)\n","        right = Input(batch_shape=shape)\n","        print(shape)\n","        # left image as reference\n","        x = Conv2D(filters=16, kernel_size=5, padding='same')(left)\n","        xleft = Conv2D(filters=1,\n","                       kernel_size=5,\n","                       padding='same',\n","                       dilation_rate=2)(left)\n","\n","        # left and right images for disparity estimation\n","        xin = keras.layers.concatenate([left, right])\n","        xin = Conv2D(filters=32, kernel_size=5, padding='same')(xin)\n","\n","        # image reduced by 8\n","        x8 = MaxPooling2D(8)(xin)\n","        x8 = BatchNormalization()(x8)\n","        x8 = Activation('relu', name='downsampled_stereo')(x8)\n","\n","        dilation_rate = 1\n","        y = x8\n","        # correspondence network\n","        # parallel cnn at increasing dilation rate\n","        for i in range(4):\n","            a = Conv2D(filters=32,\n","                       kernel_size=5,\n","                       padding='same',\n","                       dilation_rate=dilation_rate)(x8)\n","            a = Dropout(dropout)(a)\n","            y = keras.layers.concatenate([a, y])\n","            dilation_rate += 1\n","\n","        dilation_rate = 1\n","        x = MaxPooling2D(8)(x)\n","        # disparity network\n","        # dense interconnection inspired by DenseNet\n","        for i in range(4):\n","            x = keras.layers.concatenate([x, y])\n","            y = BatchNormalization()(x)\n","            y = Activation('relu')(y)\n","            y = Conv2D(filters=64,\n","                       kernel_size=1,\n","                       padding='same')(y)\n","\n","            y = BatchNormalization()(y)\n","            y = Activation('relu')(y)\n","            y = Conv2D(filters=16,\n","                       kernel_size=5,\n","                       padding='same',\n","                       dilation_rate=dilation_rate)(y)\n","            y = Dropout(dropout)(y)\n","            dilation_rate += 1\n","        \n","        # disparity estimate scaled back to original image size\n","        x = keras.layers.concatenate([x, y], name='upsampled_disparity')\n","        x = BatchNormalization()(x)\n","        x = Activation('relu')(x)\n","        x = Conv2D(filters=32, kernel_size=1, padding='same')(x)\n","        x = UpSampling2D(8)(x)\n","        if not self.settings.nopadding:\n","            x = ZeroPadding2D(padding=(2, 0))(x)\n","\n","        # left image skip connection to disparity estimate\n","        x = keras.layers.concatenate([x, xleft])\n","        y = BatchNormalization()(x)\n","        y = Activation('relu')(y)\n","        y = Conv2D(filters=16, kernel_size=5, padding='same')(y)\n","\n","        x = keras.layers.concatenate([x, y])\n","        y = BatchNormalization()(x)\n","        y = Activation('relu')(y)\n","        y = Conv2DTranspose(filters=1, kernel_size=9, padding='same')(y)\n","\n","        # prediction\n","\n","        yout = Activation('sigmoid', name='disparity_output')(y)\n","\n","        # densemapnet model\n","        self.model = Model([left, right],yout)\n","       \n","        if self.settings.model_weights:\n","            print(\"Loading checkpoint model weights %s....\"\n","                  % self.settings.model_weights)\n","            self.model.load_weights(self.settings.model_weights)\n","\n","\n","\n","        self.model.compile(loss='mse', optimizer=Adam())\n","\n","        print(\"DenseMapNet Model:\")\n","        self.model.summary()\n","        plot_model(self.model, to_file='densemapnet.png', show_shapes=True)\n","\n","        return self.model\n","print('model loaded')"],"execution_count":27,"outputs":[{"output_type":"stream","text":["model loaded\n"],"name":"stdout"}]},{"metadata":{"id":"IGpid3p5C0bh","colab_type":"text"},"cell_type":"markdown","source":["# 2.3 arrange settings and run network"]},{"metadata":{"id":"S5-QAmopipJ8","colab_type":"code","outputId":"47e5259a-9014-416e-9114-c5f06c285ce2","executionInfo":{"status":"error","timestamp":1550840482990,"user_tz":-120,"elapsed":65065,"user":{"displayName":"Shai Weisman","photoUrl":"","userId":"16331474504044317514"}},"colab":{"base_uri":"https://localhost:8080/","height":4179}},"cell_type":"code","source":["\n","\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","import random\n","import keras\n","from sklearn.utils import shuffle\n","from keras.callbacks import ModelCheckpoint, LambdaCallback\n","from keras.optimizers import RMSprop, SGD,Adam\n","import tensorflow as tf\n","import numpy as np\n","from keras import backend as K\n","import argparse\n","import os\n","from os import path\n","import time\n","import matplotlib.image as img\n","import matplotlib.pyplot as plt\n","from scipy import misc\n","\n","\n","class Predictor(object):\n","    def __init__(self, settings=Settings()):\n","        settings.nopadding = False\n","        self.settings = settings\n","        self.mkdir_images()\n","        \n","        self.get_max_disparity()\n","        self.load_test_data()\n","        \n","        self.network  = None\n","        self.train_data_loaded = False\n","        self.train_lx=np.full((144,540, 960, 3),0).astype('float32')\n","        self.train_rx=np.full((144, 540, 960, 3),0).astype('float32')\n","        self.train_dx=np.full((144,540, 960, 1),0).astype('float32')\n","        self.predicted=np.full((17,540, 960, 1),0).astype('float32')\n","        if self.settings.epe:\n","            self.best_epe = self.settings.epe\n","        else:\n","            self.best_epe = 100.0\n","\n"," \n","\n","    def load_test_disparity(self):\n","        filename =os.path.join(self.settings.pdir, \"testD.npz\")\n","        print(\"Loading... \", filename)\n","        self.test_dx=np.load( filename)['arr_0']\n","\n","        self.dmax =  max(self.dmax, np.amax(self.test_dx))\n","        self.dmin =  min(self.dmin, np.amin(self.test_dx))\n","        self.sum += np.sum(self.test_dx, axis=0)\n","        self.size += self.test_dx.shape[0]\n","        print(\"Max disparity on entire dataset: \", self.dmax)\n","        print(\"Min disparity on entire dataset: \", self.dmin)\n","        if self.settings.predict:\n","            filename =os.path.join(self.settings.pdir, \"testD.npz\" )\n","            print(\"Loading... \", filename)\n","            self.train_dx=np.load( filename)['arr_0']\n","\n","            \n","\n","        dim = self.test_dx.shape[1] * self.test_dx.shape[2]\n","        self.ave = np.sum(self.sum / self.size) / dim\n","        print(\"Ave disparity: \", self.ave)\n","        self.test_dx = self.test_dx.astype('float32') / self.dmax\n","        print(\"Scaled disparity max: \", np.amax(self.test_dx))\n","        print(\"Scaled disparity min: \", np.amin(self.test_dx))\n","\n","\n","        shape = [-1, self.test_dx.shape[1], self.test_dx.shape[2], 1]\n","        self.test_dx = np.reshape(self.test_dx, shape)\n","\n","    def get_max_disparity(self):\n","        self.dmax = 0\n","        self.dmin = 255\n","        self.sum = None\n","        self.size = 0\n","        filename =os.path.join(self.settings.pdir, \"dataD.npz\")\n","        print(filename)\n","     \n","        \n","        Data=np.load( filename)\n","       \n","        count =  1\n","\n","        filename =os.path.join(self.settings.pdir, \"dataD.npz\")\n","        print(\"Loading... \", filename)\n","#             \n","        self.train_dx=np.load( filename)['arr_0']\n","\n","        self.dmax =  max(self.dmax, np.amax(self.train_dx))\n","        self.dmin =  min(self.dmin, np.amin(self.train_dx))\n","        if self.sum is None:\n","          self.sum = np.sum(self.train_dx, axis=0)\n","        else:\n","          self.sum += np.sum(self.train_dx, axis=0)\n","        self.size += self.train_dx.shape[0]\n","        self.load_test_disparity()\n","\n","    def load_test_data(self):\n","        filename =os.path.join(self.settings.pdir, \"testL.npz\")\n","        print(\"Loading... \", filename)\n","        self.test_lx=np.load( filename)['arr_0']\n","        self.test_lx=self.test_lx/np.max(self.test_lx)\n","\n","            \n","        filename =os.path.join(self.settings.pdir, \"testR.npz\" )\n","        print(\"Loading... \", filename)\n","        self.test_rx=np.load( filename)['arr_0']\n","        self.test_rx=self.test_rx/np.max(self.test_rx)\n","\n","\n","\n","        self.channels = self.settings.channels = self.test_lx.shape[3]\n","        self.xdim = self.settings.xdim = self.test_lx.shape[2]\n","        self.ydim = self.settings.ydim = self.test_lx.shape[1]\n","\n","    def load_train_data(self, index):\n","\n","        print(index)\n","        filename =os.path.join(self.settings.pdir, \"dataL.npz\")\n","        print(\"Loading... \", filename)\n","        self.train_lx=np.load( filename)['arr_0']\n","        self.train_lx=self.train_lx/np.max(self.train_lx)\n","        filename =os.path.join(self.settings.pdir, \"dataR.npz\")\n","        print(\"Loading... \", filename)\n","        train_rx=np.load( filename)['arr_0']\n","        self.train_rx=self.train_rx/np.max(self.train_rx)\n","        filename =os.path.join(self.settings.pdir, \"dataD.npz\") \n","        tempDisp=np.load( filename)['arr_0']\n","        tempDisp = tempDisp.astype('float32') / self.dmax\n","        print(\"Scaled disparity max: \", np.amax(tempDisp))\n","        print(\"Scaled disparity min: \", np.amin(tempDisp))\n","        print(tempDisp.shape)\n","        self.train_dx =tempDisp       \n","        self.channels = self.settings.channels = 3\n","        self.xdim = self.settings.xdim = tempDisp.shape[2]\n","        self.ydim = self.settings.ydim = tempDisp.shape[1]\n","        self.train_data_loaded = True\n","\n","    def train_network(self):\n","        self.train_all()\n","         \n","\n","\n","\n","    def train_all(self, epochs=200, lr=1e-3):\n","        checkdir = \"checkpoint\"\n","        try:\n","            os.mkdir(checkdir)\n","        except FileExistsError:\n","            print(\"Folder exists: \", checkdir)\n","        \n","\n","        checkpoint = ModelCheckpoint(filepath=os.path.join(self.settings.Weights_pdir,'weights{epoch:03d}.h5'),\n","                                     save_weights_only=True,\n","                                     verbose=1,\n","                                     save_best_only=False)\n","        predict_callback = LambdaCallback(on_epoch_end=lambda epoch,\n","                                          logs: self.predict_disparity(epoch=epoch))\n","        \n","        Callbacks = [checkpoint, predict_callback]\n","        self.load_train_data(1)\n","        if self.network is None:\n","            self.network = DenseMapNet(settings=self.settings)\n","            self.model = self.network.build_model(lr=lr)\n","        print(\"Using loss=crossent on sigmoid output layer\")\n","        self.model.compile(loss=self.settings.loss_function, optimizer=Adam())\n","\n","        if self.settings.model_weights:\n","            if self.settings.notrain:\n","                self.predict_disparity()\n","                return\n","\n","        \n","        \n","        print(self.train_lx.shape)\n","        x = [self.train_lx, self.train_rx]\n","        x_test=[self.test_lx, self.test_rx]\n","        hist=self.model.fit(x,\n","                       self.train_dx,\n","                       validation_data=(x_test, self.test_dx),\n","                       epochs=self.settings.epochs,\n","                       batch_size=4,\n","                       shuffle=True,\n","                       callbacks=[checkpoint, predict_callback],\n","                            verbose=1)\n","        Path_weight=os.path.join(self.settings.Weights_pdir,'Final_weight.h5')\n","        print(Path_weight)\n","        self.model.save_weights(Path_weight)\n","    \n","\n","    def mkdir_images(self):\n","        pdir = self.settings.images_pdir\n","\n","        for dirname in [\"train\", \"test\"]:\n","            cdir = os.path.join(pdir, dirname)\n","            filepath = os.path.join(cdir, \"left\")\n","            os.makedirs(filepath, exist_ok=True)\n","            filepath = os.path.join(cdir, \"right\")\n","            os.makedirs(filepath, exist_ok=True)\n","            filepath = os.path.join(cdir, \"disparity\")\n","            os.makedirs(filepath, exist_ok=True)\n","            filepath = os.path.join(cdir, \"prediction\")\n","            os.makedirs(filepath, exist_ok=True)\n","\n","\n","    def get_epe(self, use_train_data=False, get_performance=True, epoch=77):\n","        if use_train_data:\n","            lx = self.train_lx\n","            rx = self.train_rx\n","            dx = self.train_dx\n","            print(\"Using train data... Size: \", lx.shape[0])\n","        else:\n","            lx = self.test_lx\n","            rx = self.test_rx\n","            dx = self.test_dx\n","\n","            if self.settings.predict:\n","                print(\"Using complete data... Size: \", lx.shape[0])\n","            else:\n","                print(\"Using test data... Size: \", lx.shape[0])\n","\n","        # sum of all errors (normalized)\n","        epe_total = 0\n","        # count of images\n","        t = 0\n","        nsamples = lx.shape[0]\n","        elapsed_total = 0.0\n","        if self.settings.images:\n","            print(\"Saving images on folder...\")\n","        for i in range(0, nsamples, 1):\n","            indexes = np.arange(i, i + 1)\n","            left_images = lx[indexes, :, :, : ]\n","            right_images = rx[indexes, :, :, : ]\n","            disparity_images = dx[indexes, :, :, : ]\n","\n","            # measure the speed of prediction on the 10th sample to avoid variance\n","            if get_performance:\n","                start_time = time.time()\n","                predicted_disparity = self.model.predict([left_images, right_images])\n","                elapsed_total += (time.time() - start_time)\n","            else:\n","                predicted_disparity = self.model.predict([left_images, right_images])\n","\n","            self.predicted[indexes,:,:,:] = predicted_disparity[0, :, :, :]\n","            predicted= predicted_disparity[0, :, :, :]\n","           \n","            ground = disparity_images[0, :, :, :]\n","\n","            dim = predicted.shape[0] * predicted.shape[1]\n","            epe = predicted - ground\n","            # normalized error on all pixels\n","            epe = np.sum(np.absolute(epe))\n","            \n","            epe = epe / dim\n","            epe_total += epe\n","            if (epoch%10) == 0:\n","\n","                path = \"test\"\n","                if use_train_data:\n","                    path = \"train\"\n","                filepath  = os.path.join(self.settings.images_pdir, path)\n","                left = os.path.join(filepath, \"left\")\n","                right = os.path.join(filepath, \"right\")\n","                disparity = os.path.join(filepath, \"disparity\")\n","                prediction = os.path.join(filepath, \"prediction\")\n","                print(prediction)\n","                filename = 'Image{0}.png'.format(epoch)\n","                left = os.path.join(left, filename)\n","                if left_images[0].shape[2] == 1:\n","                    self.predict_images(left_images[0], left)\n","                else:\n","                    plt.imsave(left, left_images[0])\n","\n","                right = os.path.join(right, filename)\n","                if right_images[0].shape[2] == 1:\n","                    self.predict_images(right_images[0], right)\n","                else:\n","                    plt.imsave(right, right_images[0])\n","                self.predict_images(predicted, os.path.join(prediction, filename))\n","                self.predict_images(ground, os.path.join(disparity, filename))\n","\n","        epe = epe_total / nsamples \n","        # epe in pix units\n","        epe = epe * self.dmax\n","\n","        print(\"EPE: %0.2fpix\" % epe)\n","        if epe < self.best_epe:\n","            self.best_epe = epe\n","            print(\"------------------- BEST EPE : %f ---------------------\" % epe)\n","            tmpdir = \"tmp\"\n","            try:\n","                os.mkdir(tmpdir)\n","            except FileExistsError:\n","                a=3#print(\"Folder exists: \", tmpdir)\n","            filename = open('tmp/epe.txt', 'a')\n","            datetime = time.strftime(\"%H:%M:%S\")\n","            filename.write(\"%s : EPE: %f\\n\" % (datetime,  epe))\n","            filename.close()\n","        # speed in sec\n","        if get_performance:\n","            print(\"Speed: %0.4fsec\" % (elapsed_total / nsamples))\n","            print(\"Speed: %0.4fHz\" % (nsamples / elapsed_total))\n","\n","    def predict_images(self, image, filepath):\n","        size = [image.shape[0], image.shape[1]]\n","\n","        image =  np.clip(image, 0.0, 1.0)\n","        image *= 255\n","\n","        image = image.astype(np.uint8)\n","        \n","        image = np.reshape(image, size)\n","        imageio.imwrite(filepath, image)\n","\n","    def predict_disparity(self,epoch=777):\n","        if self.settings.predict:\n","            if self.network is None:\n","                self.network = DenseMapNet(settings=self.settings)\n","                self.model = self.network.build_model()\n","            # gpu is slow in prediction during initial load of data\n","            # distorting the true speed of the network\n","            # we get the speed after 1 prediction\n","            if self.settings.images:\n","                self.get_epe(use_train_data=False, get_performance=True, epoch=epoch)\n","            else:\n","                for i in range(4):\n","                    self.get_epe(use_train_data=False, get_performance=True, epoch=epoch)\n","        else:\n","            # self.settings.images = True\n","            self.get_epe(use_train_data=False, get_performance=True, epoch=epoch)\n","            # self.get_epe(use_train_data=False)\n","            if self.settings.notrain:\n","                self.get_epe()\n","\n","\n","if __name__ == '__main__':\n","\n","    \n","    \n","    settings = Settings()\n","\n","    settings.pdir=\"/content/gdrive/My Drive/DeepLearning/Project_submission/DenseMap/Dataset\"\n","    #define if you want to train\n","    settings.notrain = False\n","    settings.loss_function=\"mse\"\n","    settings.epochs=300                      \n","    #insert Model Weights (for predict)\n","    settings.model_weights =\"\"#\"/content/gdrive/My Drive/Stereo_1/DenseMapNet/DataFamily/CosHweights285.h5\"\n","    \n","    #directory to save weights\n","    settings.Weights_pdir= \"/content/gdrive/My Drive/Stereo_1/DenseMapNet/DataFamily/Bin_Weights\"\n","\n","    settings.num_dataset = 1\n","    settings.predict = False\n","    \n","    #Mark if you want to export images and if so, to which directory\n","    settings.images = True \n","    settings.images_pdir = \"/content/gdrive/My Drive/Stereo_1/DenseMapNet/DataFamily/Bin/images\"\n","    \n","\n","    settings.epe = True\n","   \n","\n","    \n","    \n","\n","    predictor = Predictor(settings=settings)\n","    if settings.predict:\n","        predictor.predict_disparity()\n","        Output=predictor.predicted\n","    else:\n","        predictor.train_network()\n"],"execution_count":28,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/DeepLearning/Project_submission/DenseMap/Dataset/dataD.npz\n","Loading...  /content/gdrive/My Drive/DeepLearning/Project_submission/DenseMap/Dataset/dataD.npz\n","Loading...  /content/gdrive/My Drive/DeepLearning/Project_submission/DenseMap/Dataset/testD.npz\n","Max disparity on entire dataset:  0.0045319297\n","Min disparity on entire dataset:  1.5259022e-05\n","Ave disparity:  0.0006912775981573411\n","Scaled disparity max:  0.9898989\n","Scaled disparity min:  0.0033670033\n","Loading...  /content/gdrive/My Drive/DeepLearning/Project_submission/DenseMap/Dataset/testL.npz\n","Loading...  /content/gdrive/My Drive/DeepLearning/Project_submission/DenseMap/Dataset/testR.npz\n","1\n","Loading...  /content/gdrive/My Drive/DeepLearning/Project_submission/DenseMap/Dataset/dataL.npz\n","Loading...  /content/gdrive/My Drive/DeepLearning/Project_submission/DenseMap/Dataset/dataR.npz\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:129: RuntimeWarning: invalid value encountered in true_divide\n"],"name":"stderr"},{"output_type":"stream","text":["Scaled disparity max:  1.0\n","Scaled disparity min:  0.0033670033\n","(144, 540, 960, 1)\n","(None, 540, 960, 3)\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","DenseMapNet Model:\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_11 (InputLayer)           (None, 540, 960, 3)  0                                            \n","__________________________________________________________________________________________________\n","input_12 (InputLayer)           (None, 540, 960, 3)  0                                            \n","__________________________________________________________________________________________________\n","concatenate_7 (Concatenate)     (None, 540, 960, 6)  0           input_11[0][0]                   \n","                                                                 input_12[0][0]                   \n","__________________________________________________________________________________________________\n","conv2d_37 (Conv2D)              (None, 540, 960, 32) 4832        concatenate_7[0][0]              \n","__________________________________________________________________________________________________\n","max_pooling2d_1 (MaxPooling2D)  (None, 67, 120, 32)  0           conv2d_37[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_34 (BatchNo (None, 67, 120, 32)  128         max_pooling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","downsampled_stereo (Activation) (None, 67, 120, 32)  0           batch_normalization_34[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_38 (Conv2D)              (None, 67, 120, 32)  25632       downsampled_stereo[0][0]         \n","__________________________________________________________________________________________________\n","conv2d_39 (Conv2D)              (None, 67, 120, 32)  25632       downsampled_stereo[0][0]         \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 67, 120, 32)  0           conv2d_38[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_40 (Conv2D)              (None, 67, 120, 32)  25632       downsampled_stereo[0][0]         \n","__________________________________________________________________________________________________\n","dropout_2 (Dropout)             (None, 67, 120, 32)  0           conv2d_39[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_8 (Concatenate)     (None, 67, 120, 64)  0           dropout_1[0][0]                  \n","                                                                 downsampled_stereo[0][0]         \n","__________________________________________________________________________________________________\n","conv2d_41 (Conv2D)              (None, 67, 120, 32)  25632       downsampled_stereo[0][0]         \n","__________________________________________________________________________________________________\n","dropout_3 (Dropout)             (None, 67, 120, 32)  0           conv2d_40[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_9 (Concatenate)     (None, 67, 120, 96)  0           dropout_2[0][0]                  \n","                                                                 concatenate_8[0][0]              \n","__________________________________________________________________________________________________\n","conv2d_35 (Conv2D)              (None, 540, 960, 16) 1216        input_11[0][0]                   \n","__________________________________________________________________________________________________\n","dropout_4 (Dropout)             (None, 67, 120, 32)  0           conv2d_41[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_10 (Concatenate)    (None, 67, 120, 128) 0           dropout_3[0][0]                  \n","                                                                 concatenate_9[0][0]              \n","__________________________________________________________________________________________________\n","max_pooling2d_2 (MaxPooling2D)  (None, 67, 120, 16)  0           conv2d_35[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_11 (Concatenate)    (None, 67, 120, 160) 0           dropout_4[0][0]                  \n","                                                                 concatenate_10[0][0]             \n","__________________________________________________________________________________________________\n","concatenate_12 (Concatenate)    (None, 67, 120, 176) 0           max_pooling2d_2[0][0]            \n","                                                                 concatenate_11[0][0]             \n","__________________________________________________________________________________________________\n","batch_normalization_35 (BatchNo (None, 67, 120, 176) 704         concatenate_12[0][0]             \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 67, 120, 176) 0           batch_normalization_35[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_42 (Conv2D)              (None, 67, 120, 64)  11328       activation_1[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_36 (BatchNo (None, 67, 120, 64)  256         conv2d_42[0][0]                  \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 67, 120, 64)  0           batch_normalization_36[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_43 (Conv2D)              (None, 67, 120, 16)  25616       activation_2[0][0]               \n","__________________________________________________________________________________________________\n","dropout_5 (Dropout)             (None, 67, 120, 16)  0           conv2d_43[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_13 (Concatenate)    (None, 67, 120, 192) 0           concatenate_12[0][0]             \n","                                                                 dropout_5[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_37 (BatchNo (None, 67, 120, 192) 768         concatenate_13[0][0]             \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 67, 120, 192) 0           batch_normalization_37[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_44 (Conv2D)              (None, 67, 120, 64)  12352       activation_3[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_38 (BatchNo (None, 67, 120, 64)  256         conv2d_44[0][0]                  \n","__________________________________________________________________________________________________\n","activation_4 (Activation)       (None, 67, 120, 64)  0           batch_normalization_38[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_45 (Conv2D)              (None, 67, 120, 16)  25616       activation_4[0][0]               \n","__________________________________________________________________________________________________\n","dropout_6 (Dropout)             (None, 67, 120, 16)  0           conv2d_45[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_14 (Concatenate)    (None, 67, 120, 208) 0           concatenate_13[0][0]             \n","                                                                 dropout_6[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_39 (BatchNo (None, 67, 120, 208) 832         concatenate_14[0][0]             \n","__________________________________________________________________________________________________\n","activation_5 (Activation)       (None, 67, 120, 208) 0           batch_normalization_39[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_46 (Conv2D)              (None, 67, 120, 64)  13376       activation_5[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_40 (BatchNo (None, 67, 120, 64)  256         conv2d_46[0][0]                  \n","__________________________________________________________________________________________________\n","activation_6 (Activation)       (None, 67, 120, 64)  0           batch_normalization_40[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_47 (Conv2D)              (None, 67, 120, 16)  25616       activation_6[0][0]               \n","__________________________________________________________________________________________________\n","dropout_7 (Dropout)             (None, 67, 120, 16)  0           conv2d_47[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_15 (Concatenate)    (None, 67, 120, 224) 0           concatenate_14[0][0]             \n","                                                                 dropout_7[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_41 (BatchNo (None, 67, 120, 224) 896         concatenate_15[0][0]             \n","__________________________________________________________________________________________________\n","activation_7 (Activation)       (None, 67, 120, 224) 0           batch_normalization_41[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_48 (Conv2D)              (None, 67, 120, 64)  14400       activation_7[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_42 (BatchNo (None, 67, 120, 64)  256         conv2d_48[0][0]                  \n","__________________________________________________________________________________________________\n","activation_8 (Activation)       (None, 67, 120, 64)  0           batch_normalization_42[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_49 (Conv2D)              (None, 67, 120, 16)  25616       activation_8[0][0]               \n","__________________________________________________________________________________________________\n","dropout_8 (Dropout)             (None, 67, 120, 16)  0           conv2d_49[0][0]                  \n","__________________________________________________________________________________________________\n","upsampled_disparity (Concatenat (None, 67, 120, 240) 0           concatenate_15[0][0]             \n","                                                                 dropout_8[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_43 (BatchNo (None, 67, 120, 240) 960         upsampled_disparity[0][0]        \n","__________________________________________________________________________________________________\n","activation_9 (Activation)       (None, 67, 120, 240) 0           batch_normalization_43[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_50 (Conv2D)              (None, 67, 120, 32)  7712        activation_9[0][0]               \n","__________________________________________________________________________________________________\n","up_sampling2d_1 (UpSampling2D)  (None, 536, 960, 32) 0           conv2d_50[0][0]                  \n","__________________________________________________________________________________________________\n","zero_padding2d_8 (ZeroPadding2D (None, 540, 960, 32) 0           up_sampling2d_1[0][0]            \n","__________________________________________________________________________________________________\n","conv2d_36 (Conv2D)              (None, 540, 960, 1)  76          input_11[0][0]                   \n","__________________________________________________________________________________________________\n","concatenate_16 (Concatenate)    (None, 540, 960, 33) 0           zero_padding2d_8[0][0]           \n","                                                                 conv2d_36[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_44 (BatchNo (None, 540, 960, 33) 132         concatenate_16[0][0]             \n","__________________________________________________________________________________________________\n","activation_10 (Activation)      (None, 540, 960, 33) 0           batch_normalization_44[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_51 (Conv2D)              (None, 540, 960, 16) 13216       activation_10[0][0]              \n","__________________________________________________________________________________________________\n","concatenate_17 (Concatenate)    (None, 540, 960, 49) 0           concatenate_16[0][0]             \n","                                                                 conv2d_51[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_45 (BatchNo (None, 540, 960, 49) 196         concatenate_17[0][0]             \n","__________________________________________________________________________________________________\n","activation_11 (Activation)      (None, 540, 960, 49) 0           batch_normalization_45[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_transpose_1 (Conv2DTrans (None, 540, 960, 1)  3970        activation_11[0][0]              \n","__________________________________________________________________________________________________\n","disparity_output (Activation)   (None, 540, 960, 1)  0           conv2d_transpose_1[0][0]         \n","==================================================================================================\n","Total params: 293,110\n","Trainable params: 290,290\n","Non-trainable params: 2,820\n","__________________________________________________________________________________________________\n","Using loss=crossent on sigmoid output layer\n","(144, 540, 960, 3)\n","Train on 144 samples, validate on 17 samples\n","Epoch 1/300\n","124/144 [========================>.....] - ETA: 5s - loss: 0.0153"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-41fe5b46e3b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0mOutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-28-41fe5b46e3b0>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-28-41fe5b46e3b0>\u001b[0m in \u001b[0;36mtrain_all\u001b[0;34m(self, epochs, lr)\u001b[0m\n\u001b[1;32m    186\u001b[0m                        \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                        \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                             verbose=1)\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0mPath_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWeights_pdir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Final_weight.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"i982D5dOvnz6","colab_type":"text"},"cell_type":"markdown","source":["#3.JF filter\n","\n","3.1Import"]},{"metadata":{"id":"XMg-8uqpvmIa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"11d2925c-d805-48ec-e546-f8884ae3a0f9","executionInfo":{"status":"ok","timestamp":1550836729027,"user_tz":-120,"elapsed":3694,"user":{"displayName":"Shai Weisman","photoUrl":"","userId":"16331474504044317514"}}},"cell_type":"code","source":["import os # os specific actions\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import cv2 # image processing\n","import matplotlib.pyplot as plt # plots\n","import random\n","# deep learning framework\n","import keras.backend as K\n","from keras import utils\n","from keras.models import Model\n","from keras.layers import *\n","from keras.optimizers import Adam,RMSprop\n","from keras.regularizers import l2\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.callbacks import *\n","from keras import initializers\n","#patches\n","from skimage import data, util\n","from skimage.measure import label, regionprops\n","import matplotlib.patches as mpatches\n","from PIL import Image\n","from sklearn.feature_extraction import image"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"HbU_vUNsw8eC","colab_type":"text"},"cell_type":"markdown","source":["#3.2 define settings"]},{"metadata":{"id":"1mavfr66xB70","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"107d9902-860b-47aa-d444-becbe4ae6db4","executionInfo":{"status":"ok","timestamp":1550838237961,"user_tz":-120,"elapsed":1192,"user":{"displayName":"Shai Weisman","photoUrl":"","userId":"16331474504044317514"}}},"cell_type":"code","source":["activFunc=ELU()\n","Dropout_active=False\n","DO_val=0.5\n","\n","Create_train_data=True\n","Train_size=50000\n","Test_size=5000\n","#for External inputs (for internal, just put in Guide,Targe and GroundTruth the correct arrays)\n","Guide_filePath =\"/content/gdrive/My Drive/DeepLearning/Project_submission/DenseMap/Dataset/testL.npz\"\n","Guide=np.load( Guide_filePath)['arr_0']\n","Target_filePath=\"/content/gdrive/My Drive/DeepLearning/Project_submission/DenseMap/Dataset/TestOutput.npz\"\n","Target=np.load(Target_filePath)['arr_0']\n","GRoundTruth_filePath =\"/content/gdrive/My Drive/DeepLearning/Project_submission/DenseMap/Dataset/testD.npz\"\n","GroundTruth=np.load( GRoundTruth_filePath)['arr_0']\n","\n","if not Create_train_data:\n","  Guide_train_filePath='/content/gdrive/My Drive/Stereo_2/JF_dataset/Train64Guide.npz'\n","  Guide_test_filePath='/content/gdrive/My Drive/Stereo_2/JF_dataset/Test64Guide.npz'\n","  Target_train_filePath='/content/gdrive/My Drive/Stereo_2/JF_dataset/Train64Target.npz'\n","  Target_test_filePath='/content/gdrive/My Drive/Stereo_2/JF_dataset/Test64Target.npz'\n","  GRoundTruth_train_filePath='/content/gdrive/My Drive/Stereo_2/JF_dataset/Train64OutPut.npz'\n","  GRoundTruth__test_filePath='/content/gdrive/My Drive/Stereo_2/JF_dataset/TestOut64Put.npz'\n","\n","\n","\n","predict=False\n","GuidePath2Predict =\"/content/gdrive/My Drive/Stereo_2/Data/testL.npz\"\n","GT2Predict =\"/content/gdrive/My Drive/Stereo_2/Data/testD.npz\"\n","TargetPath2Predict =\"/content/gdrive/My Drive/Stereo_2/Data/TestOutput.npz\"\n","\n","path_save_weight='/content/gdrive/My Drive/Stereo_2/test/moo.h5'\n","path_save_weight_checkPoint='/content/gdrive/My Drive/Stereo_2/test/moo2.h5'\n","path_load_weight='/content/gdrive/My Drive/DeepLearning/Project_submission/JF_Net/Weights/MSE/JF_MSE_ELU_NO_DO_WEIGHT10.h5'\n","\n","\n"],"execution_count":22,"outputs":[{"output_type":"stream","text":["(17, 540, 960, 3)\n"],"name":"stdout"}]},{"metadata":{"id":"FqGjXuUQ4EEQ","colab_type":"text"},"cell_type":"markdown","source":["#3.3 Create Train data"]},{"metadata":{"id":"-HwFaQqY4N0Q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":391},"outputId":"ed4a9fc5-66c1-4dc4-fba8-81889527fa5e","executionInfo":{"status":"ok","timestamp":1550838361952,"user_tz":-120,"elapsed":1418,"user":{"displayName":"Shai Weisman","photoUrl":"","userId":"16331474504044317514"}}},"cell_type":"code","source":["\n","\n","\n","\n","GroundTruth=1-GroundTruth\n","GroundTruth =  np.clip(GroundTruth, 0.0, 1.0)\n","GroundTruth *= 255\n","T_train=np.full((Train_size,64, 64, 3),0).astype('float32')\n","G_train=np.full((Train_size,64, 64, 3),0).astype('float32')\n","Y_train=np.full((Train_size,64, 64, 1),0).astype('float32')\n","G_test=np.full((Test_size,64, 64, 3),0).astype('float32')\n","T_test=np.full((Test_size,64, 64, 3),0).astype('float32')\n","Y_test=np.full((Test_size,64, 64, 1),0).astype('float32')\n","if Create_train_data:\n","      cal=0\n","      cal2=0\n","      while (cal!=Train_size):\n","                \n","                ind= random.randint(4,16)        \n","                A=GroundTruth[ind,:,:,0]\n","                i = random.choice([m for m in range(A.shape[0])])\n","                j = random.choice([m for m in range(A.shape[1])])\n","\n","                if j-32>0 and i-32>0 and j<A.shape[0]-32 and i<A.shape[1]-32 :# and  Edge_A[j][i]==1:\n","\n","                    Y_train[cal,:,:,0] =GroundTruth[ind,j-32:j+32, i-32:i+32,0]\n","                    G_train[cal,:,:,:]=Guide[ind,j-32:j+32, i-32:i+32,0:3]/np.max(Guide)\n","                    T_train[cal,:,:,0]=Target[ind,j-32:j+32, i-32:i+32,0]/np.max(Target)\n","\n","                    cal=cal+1\n","\n","      while (cal2!=Test_size):\n","                \n","                ind= random.randint(0,3)        \n","                A=GroundTruth[ind,:,:,0]\n","                i = random.choice([m for m in range(A.shape[0])])\n","                j = random.choice([m for m in range(A.shape[1])])\n","\n","                if j-32>0 and i-32>0 and j<A.shape[0]-32 and i<A.shape[1]-32 :# \n","\n","                    Y_test[cal2,:,:,0] =GroundTruth[ind,j-32:j+32, i-32:i+32,0]\n","                    G_test[cal2,:,:,:]=Guide[ind,j-32:j+32, i-32:i+32,0:3]/np.max(Guide)\n","                    T_test[cal2,:,:,0]=Target[ind,j-32:j+32, i-32:i+32,0]/np.max(Target)\n","                    cal2=cal2+1\n","                    \n","if not Create_train_data:\n","\n","  T_train=np.full((Train_size,64, 64, 1),0).astype('float32')\n","  T_test=np.full((Test_size,64, 64, 1),0).astype('float32')\n","  G_train=np.load(Guide_train_filePath)['arr_0']\n","  Y_train=np.load(GRoundTruth_train_filePath)['arr_0']\n","  temp=np.load(Target_train_filePath)['arr_0']\n","  T_train[:,:,:,0]=temp[:,:,:,0]\n","\n","  G_test=np.load(Guide_test_filePath)['arr_0']\n","  Y_test=np.load(GRoundTruth__test_filePath)['arr_0']\n","  temp=np.load(Target_test_filePath)['arr_0']\n","  T_test[:,:,:,0]=temp[:,:,:,0]"],"execution_count":24,"outputs":[{"output_type":"stream","text":["0\n","0\n","0\n","0\n","1\n","1\n","1\n","2\n","2\n","3\n","3\n","4\n","5\n","6\n","7\n","7\n","8\n","8\n","8\n","9\n","9\n","9\n"],"name":"stdout"}]},{"metadata":{"id":"vRKl4V21ASY5","colab_type":"text"},"cell_type":"markdown","source":["#3.4 Build Model"]},{"metadata":{"id":"Vlzjx5J1AUz5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1091},"outputId":"7d0a3f31-ad68-4906-96aa-2c738f2bd429","executionInfo":{"status":"ok","timestamp":1550837559167,"user_tz":-120,"elapsed":2163,"user":{"displayName":"Shai Weisman","photoUrl":"","userId":"16331474504044317514"}}},"cell_type":"code","source":["# Define model\n","\n","\n","input_layer_target = Input(shape=T_train.shape[1:]) # full size mask as GT\n","input_layer_guide = Input(shape=G_train.shape[1:])   # full size image\n","\n","print(input_layer_target.shape)\n","print(input_layer_guide.shape)\n","\n","# CNN_T\n","\n","# 1\n","T1 = Conv2D(filters=96, kernel_size=(9,9), activation=activFunc,padding='same',kernel_initializer='glorot_normal', bias_initializer='zeros')(input_layer_target)\n","\n","T1 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n","                       beta_initializer='zeros', gamma_initializer='ones', \n","                       moving_mean_initializer='zeros', \n","                       moving_variance_initializer='ones', \n","                       beta_regularizer=None, gamma_regularizer=None, \n","                       beta_constraint=None, gamma_constraint=None)(T1)\n","if Dropout_active:\n","  T1 = Dropout(DO_val)(T1)\n","\n","# 2\n","T2 = Conv2D(filters=48, kernel_size=(1,1), activation=activFunc,padding='same',kernel_initializer='glorot_normal', bias_initializer='zeros')(T1)\n","# #T2 = ZeroPadding2D(padding=1)(T2)\n","T2 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n","                       beta_initializer='zeros', gamma_initializer='ones', \n","                       moving_mean_initializer='zeros', \n","                       moving_variance_initializer='ones', \n","                       beta_regularizer=None, gamma_regularizer=None, \n","                       beta_constraint=None, gamma_constraint=None)(T2)\n","if Dropout_active:\n","  T2 = Dropout(DO_val)(T2)\n","# 3\n","T3 = Conv2D(filters=1, kernel_size=(5,5), activation=activFunc,padding='same',kernel_initializer='glorot_normal', bias_initializer='zeros')(T2)\n","# #T3 = ZeroPadding2D(padding=1)(T3)\n","T3 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n","                       beta_initializer='zeros', gamma_initializer='ones', \n","                       moving_mean_initializer='zeros', \n","                       moving_variance_initializer='ones', \n","                       beta_regularizer=None, gamma_regularizer=None, \n","                       beta_constraint=None, gamma_constraint=None)(T3)\n","\n","print(T3.shape)\n","\n","# CNN_G\n","\n","# 1\n","G1 = Conv2D(filters=96, kernel_size=(9,9), activation=activFunc,padding='same', kernel_initializer='glorot_normal', bias_initializer='zeros')(input_layer_guide)\n","#G1 = ZeroPadding2D(padding=1)(G1)\n","G1 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n","                       beta_initializer='zeros', gamma_initializer='ones', \n","                       moving_mean_initializer='zeros', \n","                       moving_variance_initializer='ones', \n","                       beta_regularizer=None, gamma_regularizer=None, \n","                       beta_constraint=None, gamma_constraint=None)(G1)\n","\n","if Dropout_active:\n","  G1 = Dropout(DO_val)(G1)\n","# 2\n","G2 = Conv2D(filters=48, kernel_size=(1,1), activation=activFunc,padding='same',kernel_initializer='glorot_normal', bias_initializer='zeros')(G1)\n","#G2 = ZeroPadding2D(padding=1)(G2)\n","G2 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n","                       beta_initializer='zeros', gamma_initializer='ones', \n","                       moving_mean_initializer='zeros', \n","                       moving_variance_initializer='ones', \n","                       beta_regularizer=None, gamma_regularizer=None, \n","                       beta_constraint=None, gamma_constraint=None)(G2)\n","if Dropout_active:\n","  G2 = Dropout(DO_val)(G2)\n","# 3\n","G3 = Conv2D(filters=1, kernel_size=(5,5), activation=activFunc,padding='same',kernel_initializer='glorot_normal', bias_initializer='zeros')(G2)\n","# G3 = ZeroPadding2D(padding=1)(G3)\n","G3 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n","                       beta_initializer='zeros', gamma_initializer='ones', \n","                       moving_mean_initializer='zeros', \n","                       moving_variance_initializer='ones', \n","                       beta_regularizer=None, gamma_regularizer=None, \n","                       beta_constraint=None, gamma_constraint=None)(G3)\n","\n","\n","print(G3.shape)\n","concatenationLayer = concatenate([T3, G3], axis=-1)\n","\n","# CNN_F\n","\n","# 1\n","F1 = Conv2D(filters=64, kernel_size=(9,9), activation=activFunc,padding='same',kernel_initializer='glorot_normal', bias_initializer='zeros')(concatenationLayer)\t\t\t\t   \n","F1 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n","                       beta_initializer='zeros', gamma_initializer='ones', \n","                       moving_mean_initializer='zeros', \n","                       moving_variance_initializer='ones', \n","                       beta_regularizer=None, gamma_regularizer=None, \n","                       beta_constraint=None, gamma_constraint=None)(F1)\n","# F1 = Dropout(DO_val)(F1)\n","# 2\n","F2 = Conv2D(filters=32, kernel_size=(1,1), activation=activFunc,padding='same',kernel_initializer='glorot_normal', bias_initializer='zeros')(F1)\n","F2 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n","                       beta_initializer='zeros', gamma_initializer='ones', \n","                       moving_mean_initializer='zeros', \n","                       moving_variance_initializer='ones', \n","                       beta_regularizer=None, gamma_regularizer=None, \n","                       beta_constraint=None, gamma_constraint=None)(F2)\n","# F2 = Dropout(DO_val)(F2)\n","\n","# 3\n","F3 = Conv2D(filters=1, kernel_size=(5,5), activation=activFunc,padding='same',kernel_initializer='glorot_normal', bias_initializer='zeros')(F2)\n","F3 = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n","                       beta_initializer='zeros', gamma_initializer='ones', \n","                       moving_mean_initializer='zeros', \n","                       moving_variance_initializer='ones', \n","                       beta_regularizer=None, gamma_regularizer=None, \n","                       beta_constraint=None, gamma_constraint=None)(F3)\n","\n","print(F3.shape)\n","# Final Merging\n","merging = concatenate([F3, input_layer_target], axis=-1)\n","\n","## DROPOUT\n","if Dropout_active:\n","  merging = Dropout(DO_val)(merging)\n","\n","# output\n","output_layer = Conv2D(filters=1, kernel_size=(1,1), activation='sigmoid', \n","                       kernel_regularizer = l2(0.0001),  use_bias=False, \n","                       kernel_initializer='he_normal')(merging)\n","\n","                                                         \n","model =  Model([input_layer_target, input_layer_guide],output_layer)\n","model.summary() # Print model\n"],"execution_count":13,"outputs":[{"output_type":"stream","text":["(?, 64, 64, 1)\n","(?, 64, 64, 3)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as ELU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n","  identifier=identifier.__class__.__name__))\n"],"name":"stderr"},{"output_type":"stream","text":["(?, 64, 64, 1)\n","(?, 64, 64, 1)\n","(?, 64, 64, 1)\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_9 (InputLayer)            (None, 64, 64, 1)    0                                            \n","__________________________________________________________________________________________________\n","input_10 (InputLayer)           (None, 64, 64, 3)    0                                            \n","__________________________________________________________________________________________________\n","conv2d_25 (Conv2D)              (None, 64, 64, 96)   7872        input_9[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_28 (Conv2D)              (None, 64, 64, 96)   23424       input_10[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_25 (BatchNo (None, 64, 64, 96)   384         conv2d_25[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_28 (BatchNo (None, 64, 64, 96)   384         conv2d_28[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_26 (Conv2D)              (None, 64, 64, 48)   4656        batch_normalization_25[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_29 (Conv2D)              (None, 64, 64, 48)   4656        batch_normalization_28[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_26 (BatchNo (None, 64, 64, 48)   192         conv2d_26[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_29 (BatchNo (None, 64, 64, 48)   192         conv2d_29[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_27 (Conv2D)              (None, 64, 64, 1)    1201        batch_normalization_26[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_30 (Conv2D)              (None, 64, 64, 1)    1201        batch_normalization_29[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_27 (BatchNo (None, 64, 64, 1)    4           conv2d_27[0][0]                  \n","__________________________________________________________________________________________________\n","batch_normalization_30 (BatchNo (None, 64, 64, 1)    4           conv2d_30[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_5 (Concatenate)     (None, 64, 64, 2)    0           batch_normalization_27[0][0]     \n","                                                                 batch_normalization_30[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_31 (Conv2D)              (None, 64, 64, 64)   10432       concatenate_5[0][0]              \n","__________________________________________________________________________________________________\n","batch_normalization_31 (BatchNo (None, 64, 64, 64)   256         conv2d_31[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_32 (Conv2D)              (None, 64, 64, 32)   2080        batch_normalization_31[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_32 (BatchNo (None, 64, 64, 32)   128         conv2d_32[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_33 (Conv2D)              (None, 64, 64, 1)    801         batch_normalization_32[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_33 (BatchNo (None, 64, 64, 1)    4           conv2d_33[0][0]                  \n","__________________________________________________________________________________________________\n","concatenate_6 (Concatenate)     (None, 64, 64, 2)    0           batch_normalization_33[0][0]     \n","                                                                 input_9[0][0]                    \n","__________________________________________________________________________________________________\n","conv2d_34 (Conv2D)              (None, 64, 64, 1)    2           concatenate_6[0][0]              \n","==================================================================================================\n","Total params: 57,873\n","Trainable params: 57,099\n","Non-trainable params: 774\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"wE_g-sS_AaQj","colab_type":"text"},"cell_type":"markdown","source":["#Train\\Load weights\n","\n","\n","\n"]},{"metadata":{"id":"hmQzO14UAZ1T","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":88},"outputId":"33c9312d-9f99-4264-9617-ce43df78e3a6","executionInfo":{"status":"ok","timestamp":1550837926359,"user_tz":-120,"elapsed":202403,"user":{"displayName":"Shai Weisman","photoUrl":"","userId":"16331474504044317514"}}},"cell_type":"code","source":["model.compile(optimizer=Adam(), loss=\"mse\" ,metrics=[ 'acc'])\n","weight_saver = ModelCheckpoint(path_save_weight_checkPoint, monitor='val_acc', \n","                                              save_best_only=True, save_weights_only=True)\n","\n","\n","early_stopping = EarlyStopping(monitor='loss', mode=\"min\", patience=5)\n","if not predict:\n","  H=model.fit([T_train,G_train],Y_train,validation_data=([T_test,G_test],Y_test),epochs=1, shuffle=True,callbacks = [weight_saver, early_stopping])\n","  model.save_weights(path_save_weight)\n","if predict:\n","  model.load_weights(path_load_weight)\n"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Train on 50000 samples, validate on 5000 samples\n","Epoch 1/1\n","50000/50000 [==============================] - 198s 4ms/step - loss: 4.6331e-04 - acc: 1.3524e-04 - val_loss: 7.1421e-04 - val_acc: 1.2451e-04\n"],"name":"stdout"}]},{"metadata":{"id":"z_nnyyUsDM9n","colab_type":"text"},"cell_type":"markdown","source":["#Create Image"]},{"metadata":{"id":"qgts9xtgDRWn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":238},"outputId":"430cd4e6-0b45-4366-97ca-3698bbaf1b96","executionInfo":{"status":"ok","timestamp":1550837651523,"user_tz":-120,"elapsed":10755,"user":{"displayName":"Shai Weisman","photoUrl":"","userId":"16331474504044317514"}}},"cell_type":"code","source":["\n","#Read Guide image\n","prev_error=0\n","post_error=0\n","num_images=1\n","for ind in range(num_images):\n"," \n","  Guide=np.full((1,540, 960, 3),0).astype('float32')\n","  temp=np.load( GuidePath2Predict)['arr_0']\n","  Guide[0,:,:,:]=temp[ind,:,:,:]\n","#Read Target image\n","\n","  target=np.full((1,540, 960,1),0).astype('float32')\n","  temp=np.load( TargetPath2Predict)['arr_0']\n","  target[0,:,:,:]=temp[ind,:,:,:]\n","\n"," \n","  Disp=np.full((1,540, 960, 1),0).astype('float32')\n","  temp=np.load( GT2Predict)['arr_0']\n","  Disp[0,:,:,:]=temp[ind,:,:,:]\n","  \n","  Output=np.full((540, 960, 3),0).astype('float32')\n","  TargetPatch=np.full((1,64, 64, 1),0).astype('float32')\n","  GuidePatch=np.full((1,64, 64, 3),0).astype('float32')\n","  for i in range(960):\n","    for j in range(540):\n","  \n","      if j-31>0 and i-31>0 and j<target.shape[1]-31 and i<target.shape[2]-31 and (j%16)==0 and (i%16)==0:\n","        TargetPatch[0,:,:,0]=target[0,j-32:j+32, i-32:i+32,0]/np.max(target)\n","\n","        GuidePatch[0,:,:,:]=Guide[0,j-32:j+32, i-32:i+32,0:3]/np.max(Guide)\n","\n","        predictPatch=model.predict([TargetPatch,GuidePatch])\n","\n","        Output[j-16:j+16, i-16:i+16,0]=predictPatch[0,16:48,16:48,0]\n","  \n","  \n","\n","  A=Disp[0,:,:,0]\n","  B=1-A\n","  B =  np.clip(B, 0.0, 1.0)\n","  B *= 255\n","  GT16=B\n","  GT16=GT16[32:508,32:928]\n","  GT16=GT16-np.min(GT16)\n","  GT16=(GT16/np.max(GT16))*254 +1\n","  T=target[0,32:508,32:928,0]\n","\n","  T=((T-np.min(T)))\n","\n","  T=(T/np.max(T))*254 +1\n","  area=GT16.shape[0]*GT16.shape[1]\n","  AbsDis=np.abs(T-GT16)\n","  errDis=AbsDis\n","  Error_T=np.sum(errDis)/area\n","  prev_error=prev_error+Error_T/num_images\n","  print(ind)\n","  print('error before JF')\n","  print(np.sum(np.abs(T-GT16))/area)\n","\n","  P=Output[32:508,32:928,0]\n","  P=((P-np.min(P)))\n","  P=(P/np.max(P))*254 +1\n","  AbsDis=np.abs(P-GT16)\n","  errDis=AbsDis\n","  Error_O=np.sum(errDis)/area\n","  post_error=post_error+Error_O/num_images\n","  print('error after JF')\n","  print(np.sum(np.abs(P-GT16))/area)\n","\n","print('//////////////////////////')  \n","print(GT16.shape[1])\n","print(GT16.shape[0])\n","print('-----------error before JF')\n","print(prev_error)\n","print('-----------error after JF')\n","print(post_error)\n","print('//////////////////////////')"],"execution_count":16,"outputs":[{"output_type":"stream","text":["0\n","error before JF\n","6.879185267857143\n","error after JF\n","2.542625546312275\n","//////////////////////////\n","896\n","476\n","-----------error before JF\n","6.879185267857143\n","-----------error after JF\n","2.542625546312275\n","//////////////////////////\n"],"name":"stdout"}]}]}